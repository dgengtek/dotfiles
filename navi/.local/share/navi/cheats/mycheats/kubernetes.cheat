% kubernetes, k8s

# display addresses of the control plane and services with label kubernetes.io/cluster-service=true
kubectl cluster-info

# set a default storageclass
kubectl patch storageclass <storageclass> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# set a default ingressclass
kubectl patch ingressclasses <ingressclass> -p '{"metadata": {"annotations":{"ingressclass.kubernetes.io/is-default-class":"true"}}}'

# get api server url
kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}'

# get authorization bearer token
kubectl get secret $(kubectl get serviceaccount default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 --decode

# delete resources  with the given state
kubectl get <resource> -o json | jq -r '.items[] | select(.status.phase == "<state>") | .metadata.name' | xargs -L 1 kubectl delete

# delete evicted pods 
kubectl get pods -A -o json | jq -r '.items[] | select(.status.phase == "Failed" and .status.reason == "Evicted") | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)' | xargs -L 1 kubectl delete

# get jobs overview with state
kubectl get jobs -A -o json | jq '.items[] | del(.spec, .kind)'

# get command string list of jobs for a given state 
kubectl get jobs -A -o json | jq -r '.items[] | select(.status.conditions[].type == "<job_state>") | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)'

# get status of failed pods
kubectl get pods -A -o json | jq '.items[] | select(.status.phase == "Failed") | .metadata.name, .metadata.namespace, .status'

# get resources which have not the given state
kubectl get <resource> -A -o json | jq -r '.items[] | select(.status.phase != "<state>") | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)'

# get containers which are not in ready state
kubectl get pods -A -o json | jq '.items[] | select(.status.phase == "Running" and .status.containerStatuses[].ready == false) | {"name": .metadata.name, "namespace": .metadata.namespace, "containers": .status.containerStatuses[]}'

# delete a single resource
kubectl delete <resource_object>

# operate on a resource object
kubectl <operation> <resource_object>

# change persistent volume policy
kubectl patch pv <pv> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'

# remove finalizers from pv
kubectl patch pv <pv> -p '{"metadata":{"finalizers":null}}'

# Print the supported API resources on the server
kubectl api-resources

# debug and diagnose cluster problems
kubectl cluster-info dump

# Print all contexts
kubectl config get-contexts

# Print current context of kubeconfig
kubectl config current-context

# Set context of kubeconfig
kubectl config use-context <context>

# Print resource documentation
kubectl explain <resource>

# display node resource
kubectl get nodes --show-labels -o wide

# Get namespaces
kubectl get namespaces

# get all crds
kubectl get crds -o json | jq -r .items[].metadata.name

# Get pods from namespace (add option '-o wide' for details)
kubectl get pods -n <namespace>

# Get pods from all namespace (add option '-o wide' for details)
kubectl get pods --all-namespaces

# allow scheduling on master
kubectl taint node <node> node-role.kubernetes.io/master:NoSchedule-

# get taints for each node
kubectl get nodes -o json | jq '.items[] | (.metadata.name , .spec)'

# restart a resource
kubectl rollout restart <resource>/<name>

# forcefully delete pod
kubectl delete <pod> --grace-period=0 --force

# wait for node
kubectl wait node/<nodename> --for=condition=Ready

# Get services from namespace
kubectl get services -n <namespace>

# Get details from resource on namespace
kubectl describe <resource>/<name> -n <namespace>

# Print logs from namespace
kubectl logs -f pods/<name> -n <namespace>

# Get deployments
kubectl get deployments -n <namespace>

# Edit deployments
kubectl edit deployment/<name> -n <namespace>

# Drain node in preparation for maintenance
kubectl drain <name>

# Mark node as schedulable
kubectl uncordon <name>

# Mark node as unschedulable
kubectl cordon <name>

# run a shell in a container on the cluster
kubectl run tmp-shell --restart=Never --rm -i --tty --image centos -- /bin/bash

# Display resource (cpu/memory/storage) usage
kubectl top <type>

# debug a pod 
pod=<pod>; podname=${pod##*/}; kubectl debug -it "$pod" --image=busybox --target="$podname"

# run a paused pod for testing
kubectl run <pod_name> --image=k8s.gcr.io/pause:3.1 --restart=Never

# run a sleeping pod for testing
kubectl run <pod_name> --image=busybox --restart=Never -- sleep 1d

# create a crashing pod
kubectl run --image=busybox <pod_name> -- false

# copy a pod and add a new container
pod=<pod>; podname=${pod##*/}; kubectl debug "$pod" -it --image=<image_name> --share-processes --copy-to="$podname-debug"

# copy a pod and change its command
pod=<pod>; podname=${pod##*/}; kubectl debug "$pod" -it --copy-to="$podname-debug" --container="$podname" --image=<image_name> -- sh

# copy a pod and change all of its containers to the image
pod="$pod"; podname=${pod##*/}; kubectl debug <pod> --copy-to="$podname-debug" --set-image=*=<image_name>

# create a debug shell on the node
kubectl debug <node> -it --image=busybox

# cleanup all replicasets except the newest
kubectl get rs -l <label> --sort-by={'.metadata.creationTimestamp'} -o name | sed '$ d' | xargs -n 1 kubectl delete rs

# get object with namespace
kubectl get <resource> -A -o json | jq -r '.items[] | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)'

# get pv volumeName storage details from pvc
kubectl describe $(kubectl get <pvc> -o go-template='{{.spec.volumeName}}'); kubectl describe pv "$volume_name"

# get pv location from a persistentvolumeclaim claimref
kubectl get pv -o json | jq -r '.items[] | select(.spec.claimRef) | (.metadata.name + " " + .spec.storageClassName + " " + .spec.capacity.storage + " " + (.spec.nodeAffinity.required.nodeSelectorTerms[].matchExpressions[].values[])? + " " + .spec.claimRef.namespace + " " + .spec.claimRef.name + " " + (if .spec.local? then (.spec.local.fsType + " " + .spec.local.path) elif .spec.nfs? then (.spec.nfs.server + ":" + .spec.nfs.path) else "" end))'

# get pv information from every persistent volume
kubectl get pv -o json | jq -r '.items[] | (.metadata.name + " " + .spec.storageClassName + " " + .spec.capacity.storage + " " + (.spec.nodeAffinity.required.nodeSelectorTerms[].matchExpressions[].values[])? + " " + .spec.claimRef.namespace + " " + .spec.claimRef.name + " " + (if .spec.local? then (.spec.local.fsType + " " + .spec.local.path) elif .spec.nfs? then (.spec.nfs.server + ":" + .spec.nfs.path) else "" end) + " " + .status.phase)'

# cleanup old signing requests by filtering for approved 
kubectl get certificatesigningrequests.certificates.k8s.io -o json | jq -r '.items[] | select(now - (.metadata.creationTimestamp | fromdate) > 432000) | select(.status.conditions[].type == "Approved") | .metadata.name' | xargs -L 1 -P $(($(nproc)*4)) kubectl delete certificatesigningrequests.certificates.k8s.io

# save secret in variable
<variable_name>=$(kubectl get <secrets_object> --template={{.data}})

# select a persistent volume object
<pv>

# select a persistent volume claim object
<pvc>

# select a pod object
<pod>

# get namespace and object name
jq -r '.items[] | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)'

# get owner of object
jq -r '.items[].metadata.ownerReferences'

# get object state
jq -r '.items[].status'

# get object containers
jq -r '.items[] | (spec.nodeName + " " + .status.containerStatuses)'

# get container id
jq -r '(.spec.nodeName + " " + .status.containerStatuses[].containerID)'

# get labels
jq '.items[].metadata.labels'

# get all service node ports
kubectl get svc --all-namespaces -o go-template='{{range .items}}{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{.}}{{"\n"}}{{end}}{{end}}{{end}}'

# get all ports from pods
kubectl get pods --all-namespaces -o go-template='{{range .items}}{{$meta:=.}}{{range .spec.containers}}{{range .ports}}{{$meta.metadata.namespace}}{{" "}}{{$meta.metadata.name}}{{" "}}{{.}}{{"\n"}}{{end}}{{end}}{{end}}'

# get all ports from a resource template(statefulset, deployment)
kubectl get <resource> --all-namespaces -o go-template='{{range .items}}{{$meta:=.}}{{range .spec.template.spec.containers}}{{range .ports}}{{$meta.metadata.namespace}}{{" "}}{{$meta.metadata.name}}{{" "}}{{.}}{{"\n"}}{{end}}{{end}}{{end}}'

# sort by age
--sort-by={.metadata.creationTimestamp}

# get all claimed volumes from all pods
kubectl get pods -A -o json | jq '[.items[] | {"claimName": (.spec.volumes[] | select(.persistentVolumeClaim) | .persistentVolumeClaim.claimName), "name": .metadata.name}]'

# group by claimed persistent volume name for each pod
{ kubectl -n ops get pods -o json | jq '.items[] | {"claimName": (.spec.volumes[] | select(.persistentVolumeClaim) | .persistentVolumeClaim.claimName), "pod": .metadata.name}'; kubectl get pv -o json | jq -r '.items[] | (select(.spec.claimRef) | {"name": .metadata.name, "class": .spec.storageClassName, "capacity": .spec.capacity.storage, "node": (.spec.nodeAffinity.required.nodeSelectorTerms[].matchExpressions[].values[]), "namespace": .spec.claimRef.namespace, "claimName": .spec.claimRef.name, "path": (if .spec.local? then .spec.local.path elif .spec.nfs? then (.spec.nfs.server + ":" + .spec.nfs.path) else "" end), "fstype": (if .spec.local.fsType? then .spec.local.fsType else "" end)})'; } | jq -s 'group_by(.claimName)[] | add'

# debug events
kubectl get events --sort-by='{.lastTimestamp}'

# create docker registry secret
kubectl create secret docker-registry regcred --docker-server='https://docker-registry.ops.svc:5000' --docker-username="cluster" --docker-password="$(systemd-ask-password)"

# get errored helm deployments 
kubectl get secrets -A -l owner=helm -o json | jq '.items[].metadata | select(.labels.status != "deployed" and .labels.status != "superseded") | (.name + " " + .namespace + " " + .labels.status)'

# rollout restart and wait for all objects in a resource
for name in $(kubectl -n <namespace> get <resource_rollout> -o jsonpath='{.items[*].metadata.name}'); do kubectl -n <namespace> rollout restart <resource_rollout> $name; kubectl -n <namespace> rollout status --watch <resource_rollout>; done

# select the active context
kubectl config use-context <context>

# Display merged kubeconfig settings or a specified kubeconfig file
kubectl config view

# Display the current-context
kubectl config current-context


$ resource: kubectl api-resources -o name --- --preview 'kubectl explain {}'
$ resource_object: kubectl get <resource> -A -o json | jq -r '.items[] | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)' --- --delimiter ' ' --preview 'kubectl describe {3} {1} {2}'
$ operation: echo -e 'get,describe,explain,edit,delete,logs' | tr ',' '\n'
# https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/
$ pod_state: echo -e 'Pending,Running,Succeeded,Failed,Unknown' | tr ',' '\n'
$ job_state: echo -e 'Complete,Failed' | tr ',' '\n'
$ resource_rollout: echo -e 'deployment,daemonset,replicaset,statefulset' | tr ',' '\n'
$ node: kubectl get nodes -o name --- --preview 'kubectl describe node {}'
$ context: kubectl config get-contexts -o name
$ namespace: kubectl get namespaces -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n'

$ pod: kubectl get pods -A -o json | jq -r '.items[] | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)' --- --delimiter ' ' --preview 'kubectl describe -n {2} {3}'
$ pvc: kubectl get pvc -A -o json | jq -r '.items[] | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)' --- --delimiter ' ' --preview 'kubectl describe -n {2} {3}'
$ pv: kubectl get pv -A -o json | jq -r '.items[] | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)' --- --delimiter ' ' --preview 'kubectl describe -n {2} {3}'
$ secrets_object: kubectl get secrets -A -o json | jq -r '.items[] | ("-n " + .metadata.namespace + " " + .kind + "/" + .metadata.name)' --- --delimiter ' ' --preview 'kubectl describe {2}'
